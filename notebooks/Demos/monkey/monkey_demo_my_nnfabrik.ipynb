{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nnfabrik, neuralpredictors, and nnvision from the sinzlab repository have to be installed/cloned\n",
    "\n",
    "- we are using the pytorch image called `sinzlab/pytorch:v3.8-torch1.4.0-cuda10.1-dj0.12.4`\n",
    "    - docker image can be found here: https://github.com/sinzlab/pytorch-docker or https://hub.docker.com/r/sinzlab/pytorch/dockerfile\n",
    "    - there, the complete list of packages to be installed can be found.\n",
    "    - torch >= 1.4 is required.\n",
    "</br>\n",
    "</br>\n",
    "- All individual pickle files and the image-pickle file have to be present. The can be found on the GPU server under /var/lib/nova/sinz-shared/data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make sure to install all required packages. dependencies are listed in the dockerfile above. If necessary, install packages within the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%okjk\n"
    }
   },
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "import os\n",
    "\n",
    "dj.config['enable_python_native_blobs'] = True\n",
    "dj.config['nnfabrik.schema_name'] = \"mburg_nnvision_monkey_demo\"\n",
    "\n",
    "dj.config[\"display.limit\"] = 50\n",
    "# set external store based on env vars\n",
    "if \"stores\" not in dj.config:\n",
    "    dj.config[\"stores\"] = {}\n",
    "dj.config[\"stores\"][\"minio\"] = {  # store in s3\n",
    "    \"protocol\": \"s3\",\n",
    "    \"endpoint\": os.environ.get(\"MINIO_ENDPOINT\", \"DUMMY_ENDPOINT\"),\n",
    "    \"bucket\": \"nnfabrik\",\n",
    "    \"location\": \"dj-store\",\n",
    "    \"access_key\": os.environ.get(\"MINIO_ACCESS_KEY\", \"FAKEKEY\"),\n",
    "    \"secret_key\": os.environ.get(\"MINIO_SECRET_KEY\", \"FAKEKEY\"),\n",
    "}\n",
    "\n",
    "from nnfabrik.main import my_nnfabrik\n",
    "my_nnf = my_nnfabrik(\"mburg_nnvision_monkey_demo\", use_common_fabrikant=False, use_common_seed=False)\n",
    "\n",
    "from nnfabrik.utility.dj_helpers import CustomSchema\n",
    "schema = CustomSchema(dj.config.get(\"nnfabrik.schema_name\", \"mburg_nnvision_monkey_demo\"))\n",
    "\n",
    "from nnfabrik.templates.trained_model import TrainedModelBase\n",
    "from nnfabrik.main import *\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from nnfabrik import builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNfabrik intro: Using the builder to build the dataloader objects, models, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's where the data is on the server:\n",
    "os.listdir('/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### loading monkey data\n",
    "\n",
    "basepath = '/data/monkey/toliaslab/CSRF19_V1'\n",
    "neuronal_data_path = os.path.join(basepath, 'neuronal_data/')\n",
    "neuronal_data_files = [neuronal_data_path+f for f in listdir(neuronal_data_path) if isfile(join(neuronal_data_path, f))]\n",
    "image_file = os.path.join(basepath, 'images/CSRF19_V1_images.pickle')\n",
    "image_cache_path = os.path.join(basepath, 'images/individual')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the dataset function: its defined in nnvision/datasets, and has to present in the __init__.py there.\n",
    "dataset_fn = 'nnvision.datasets.monkey_static_loader'\n",
    "dataset_config = dict(dataset='CSRF19_V1',\n",
    "                               neuronal_data_files=neuronal_data_files,\n",
    "                               image_cache_path=image_cache_path,\n",
    "                               crop=102,\n",
    "                               subsample=1,\n",
    "                               seed=1000,\n",
    "                               time_bins_sum=6,\n",
    "                               batch_size=128,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = builder.get_data(dataset_fn, dataset_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NNfabrik expects dataloaders to be nested dictionarys with actual PyTorch DataLoader Objects at the second Tier. The First Tier will be \"train\", \"validation\", and \"test\". The second Tier will be \"session_key\": DataLoader. So each dataset is either comprised of one or multiple sessions, with a session ID as the dictionary key to its dataloader.\n",
    "Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a random image. The images are cropped to be 20x20, so it trains fast for demo purposes.\n",
    "some_image = dataloaders[\"train\"][list(dataloaders[\"train\"].keys())[0]].dataset[:].inputs[0,0,::].cpu().numpy()\n",
    "plt.imshow(some_image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first data_key\n",
    "first_session_ID = list((dataloaders[\"train\"].keys()))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dataloader = dataloaders[\"train\"][first_session_ID]\n",
    "a_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = next(iter(a_dataloader))\n",
    "print(\"image_dimensions:\", inputs.shape)\n",
    "print(\"number of neurons of that session: \", targets.shape)\n",
    "print(\"total training batches: \", len(a_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimension\n",
    "input_shape = dataloaders[\"train\"][first_session_ID].dataset[:].inputs.shape\n",
    "print(input_shape)\n",
    "# total images = 16064\n",
    "# dims: N x C x W x H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outpt dimensions: neuronal firing rates'\n",
    "output_shape = dataloaders[\"train\"][first_session_ID].dataset[:].targets.shape\n",
    "\n",
    "print(output_shape)\n",
    "# Output: 14 Neurons, with N spikes over 60 ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models are built using the neuralpredictors repo from sinzlab. They consist of a convolutional core (with a user-specified number of layers), and a readout (spatial transformer readout, described in Sinz et al, 2018, NeurIPS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_fn = 'nnvision.models.se_core_full_gauss_readout'\n",
    "model_config = {'pad_input': False,\n",
    "                'stack': -1,\n",
    "               'depth_separable': True,\n",
    "               'input_kern': 20,\n",
    "               'gamma_input': 11.2,\n",
    "               'gamma_readout': 0.33,\n",
    "               'hidden_dilation': 1,\n",
    "               'hidden_kern': 5,\n",
    "               'n_se_blocks': 0,\n",
    "               'hidden_channels': 32}\n",
    "model = builder.get_model(model_fn, model_config, dataloaders=dataloaders,seed=1000)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the model description. Each session has its own readout. The Readout learns an x,y position between -1 and 1, relative to image space, and reads out from that point in feature space. THat means that the effective receptive field size of a unit in the last hidden layer will also be the receptive field size of the neuron. The x/y coordinates can be accessed like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.core.features.layer0.conv.weight.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show example readout positions after initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x/y position (after random initialization, becuse the model isnt trained yet) for each neuron in that session\n",
    "model.readout[first_session_ID].grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the trainer is taking care of the whole training process. when the trainer is built, it√Ñ's a function with the configuration already initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_fn = 'nnvision.training.nnvision_trainer'\n",
    "trainer_config = dict(max_iter=1, \n",
    "                      lr_decay_steps=4, \n",
    "                      tolerance=0.0005, \n",
    "                      patience=5,\n",
    "                      verbose=False, \n",
    "                      lr_init=0.003,\n",
    "                      avg_loss=False,\n",
    "                      device='cuda')\n",
    "\n",
    "trainer = builder.get_trainer(trainer_fn, trainer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, output, model_state = trainer(model=model, dataloaders=dataloaders, seed=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Two: NNfabrik and DataJoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the builder to get the data/model/and trainer, we can use datajoint to manage that process for us.\n",
    "There are Model, Dataset, and Trainer Tables. And each combination in those tables should in principle lead to a fully trained model.\n",
    "For completeness, there is also a Seed table that stores the random seed, and a Fabrikant table, that stores the name and contact details of the creator (=Fabrikant).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nnf.Fabrikant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this entry to reflect your datajoint username\n",
    "my_nnf.Fabrikant().insert1(dict(fabrikant_name='mburg',\n",
    "                         email=\"first.last@uni-goettingen.de\",\n",
    "                         affiliation='eckerlab',\n",
    "                         dj_username=\"mburg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nnf.Fabrikant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nnf.Seed().insert([{'seed':1000}])\n",
    "my_nnf.Seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  add entries for dataset, model, and trainer, with their corresponding configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds the dataset_function and dataset config that we defined above to the datase table\n",
    "my_nnf.Dataset().add_entry(dataset_fn, dataset_config, dataset_comment='CSRF_V1', dataset_fabrikant='mburg')\n",
    "my_nnf.Dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = 'nnvision.models.se_core_full_gauss_readout'\n",
    "model_config = {'pad_input': False,\n",
    "                'stack': -1,\n",
    "               'depth_separable': True,\n",
    "               'input_kern': 20,\n",
    "               'gamma_input': 11.2,\n",
    "               'gamma_readout': 0.33,\n",
    "               'hidden_dilation': 1,\n",
    "               'hidden_kern': 5,\n",
    "               'n_se_blocks': 0,\n",
    "               'hidden_channels': 32,\n",
    "               'gauss_type': 'isotropic'}                \n",
    "my_nnf.Model().add_entry(model_fn, model_config, model_comment='isotropic', model_fabrikant='mburg')\n",
    "my_nnf.Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nnf.Trainer.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_fn = 'nnvision.training.nnvision_trainer'\n",
    "trainer_config = dict(max_iter=2, \n",
    "                      lr_decay_steps=4, \n",
    "                      tolerance=0.0005, \n",
    "                      patience=5,\n",
    "                      verbose=False, \n",
    "                      lr_init=0.0045,\n",
    "                      avg_loss=False,\n",
    "                      device='cuda')\n",
    "\n",
    "my_nnf.Trainer().add_entry(trainer_fn, trainer_config, trainer_comment=\"max_iter: 2\", trainer_fabrikant='mburg')\n",
    "my_nnf.Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(my_nnf.Trainer&\"trainer_hash='e225a557fe8039df3717ddbe4686caa9'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  The TrainedModel is a template, which can be found in nnfabrik.template.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the trained model table is taking care of model training, and stores the model state in a part table. For further analyses of the trained model, one can either overwrite the TrainedModel definition by inheriting from the Base template class, or by attaching other tables to trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the simples TrainedModel class\n",
    "@my_nnf.schema\n",
    "class TrainedModel(TrainedModelBase):\n",
    "    nnfabrik = my_nnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainedModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as primary keys, it has the hashes of all the configurations, and it stores the score, and the output (which are defined in the respective trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lets populate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot graph of all tables in schema\n",
    "\n",
    "This is mainly intended as workaround for bug in dj version 0.12.9 and 0.13.0 https://github.com/datajoint/datajoint-python/issues/902"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.ERD(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainedModel().populate(display_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hash = TrainedModel().fetch1(\"model_hash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now if you want to build the model again, we can use the .load_model() function of the trained model table.\n",
    "# To use the load model function, the table needs to be restricted to one Entry. \n",
    "# for example: restricting with a key:\n",
    "some_key = dict(model_hash=model_hash)\n",
    "TrainedModel&some_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Load a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, model = (TrainedModel & some_key).load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that is the trained model, with the state dict loaded and all. lets set to eval and start using it\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's also the parameter extension, so that you can restrict with the confif objects as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfabrik.utility.dj_helpers import create_param_expansion, make_definition\n",
    "ModelExpanded = create_param_expansion('nnvision.models.se_core_full_gauss_readout', Model,fn_field='model_fn', config_field='model_config')\n",
    "ModelParams = schema(ModelExpanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelParams.populate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example:\n",
    "TrainedModel*ModelParams&\"hidden_kern=5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can just use that for building the model:\n",
    "dataloaders, model = (TrainedModel & (ModelParams & \"hidden_kern=5\")).load_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}